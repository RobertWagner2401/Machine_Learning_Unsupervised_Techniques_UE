{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Machine Learning: Unsupervised Techniques</h1>\n",
    "<h1 style=\"color:rgb(0,120,170)\">Assignment 2b: PCA</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This material, no matter whether in printed or electronic form, may be used for personal and non-commercial educational use only. Any reproduction of this material, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Task 1: Implementation of PCA for a standard data set from scratch: </h2>\n",
    "\n",
    "The goal of this task is to implement the PCA algorithm as introduced in the slides by hand. PCA analysis is a powerful tool to identify patterns in data; it is used to find correlations between variables, to determine the directions of maximum variance and project it onto a smaller dimensional subspace that maintains most of the information. To fix the overall notation, we will denote by $n$ the number of samples, by $d$ the number of different features, and $k<d$ the dimension of the space, on which we want to project. We will work with the famous iris dataset which contains measurements for $n=150$ iris flowers from three different classes, namely:\n",
    "\n",
    "* Iris-setosa ($n_{\\textrm{se}}=50$)\n",
    "* Iris-versicolor ($n_{\\textrm{ve}}=50$)\n",
    "* Iris-virginica ($n_{\\textrm{vi}}=50$).\n",
    "\n",
    "Moreover, we have the following $d=4$ features:\n",
    "\n",
    "* sepal length in cm\n",
    "* sepal width in cm\n",
    "* petal length in cm\n",
    "* petal width in cm\n",
    "\n",
    "<img src = \"../../data/iris_with_labels.jpg\">\n",
    "\n",
    "For more information on the dataset go to https://archive.ics.uci.edu/ml/datasets/Iris.\n",
    "\n",
    "In the next step we want to read in the data set using the pandas package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and reading data set. Nothing to do here.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(\n",
    "    filepath_or_buffer='iris.data', \n",
    "    header=None, \n",
    "    sep=',')\n",
    "\n",
    "df.columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
    "df.dropna(how=\"all\", inplace=True) # drops the empty line at file-end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data table into data X and class labels y. Nothing to do here.\n",
    "X = df.iloc[:,0:4].values\n",
    "y = df.iloc[:,4].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our iris dataset is now an $n \\times d=150 \\times 4$ matrix $\\mathbf{X}$, where the columns are the different features, and every row is an individual flower sample. Each sample row $\\mathbf{x}_i$ for $1\\le i \\le n=150$ is as a $4$-dimensional vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">Exercise 1 (10 points):</h3>\n",
    "\n",
    "* To get a feeling for how the 3 different flower classes are distributed along the 4 different features, visualize them via histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As PCA leads to a (sub)space that maximizes the variance along the axes, it makes sense to standardize the data, especially, if they were measured on different scales. We thus transform the data onto unit scale (mean=0 and variance=1), which is needed in many machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nothing to do here\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_std = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the covariance matrix $\\mathbf{C}$, which is a $d√ód$ matrix (real and symmetric), where each element represents the covariance between two features. $\\mathbf{C}$ can therefore be calculated as follows:\n",
    "$$\n",
    "c_{jk}=\\frac{1}{n-1} \\sum_{i=1}^{n} (x_i^{(j)}-\\overline{x}^{(j)})(x_i^{(k)}-\\overline{x}^{(k)}),\n",
    "$$\n",
    "where $\\overline{x}^{(j)}$ denotes the mean of the $j$-th feature. In matrix notation, this can be expressed as follows:\n",
    "$$\n",
    "\\mathbf{C}=\\frac{1}{n-1}(\\mathbf{X}-\\overline{\\mathbf{x}})^T (\\mathbf{X}-\\overline{\\mathbf{x}}),\n",
    "$$\n",
    "with $\\overline{\\mathbf{x}}= \\frac1n \\sum_{i=1}^{n} \\mathbf{x}_i$ denoting the mean vector. The eigenvectors and eigenvalues of $\\mathbf{C}$ represent the most important part of PCA: The eigenvectors (principal components, German: \"Hauptachsen\") determine the directions of the new feature space, and the eigenvalues contain information about their importance. Put differently, the eigenvalues explain the variance of the data along the new feature axes. Computing $\\mathbf{C}$ and its eigendecomposition should therefore be achieved in the following tasks: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">Exercise 2 (5 points):</h3>\n",
    "\n",
    "\n",
    "* Using the previous formulas and explanations, implement the covariance matrix for the iris data set and print the result. Don't use the np.cov function. Verify first that all the feature means are zero due to the standardization we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">Exercise 3 (5 points):</h3>\n",
    "\n",
    "* Next, perform an eigendecomposition on the covariance matrix  and print the eigenvectors and eigenvalues. You can use whatever method you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To decide which eigenvector(s) can safely be dropped, we take a more careful look at the corresponding eigenvalues: In order to do so, we rank the eigenvalues from highest to lowest and choose the top $k$ eigenvectors. The eigenvectors with the lowest eigenvalues contain the least information about the distribution of the data; these are the ones we will get rid of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">Exercise 4 (10 points):</h3>\n",
    "\n",
    "* Make a list of (eigenvalue, eigenvector) tuples, sort the (eigenvalue, eigenvector) tuples from high to low (with respect to the eigenvalues) and print the tuples in decreasing order to confirm, that the list is correctly sorted. Also check that the eigenvectors are all normalized and all orthogonal to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After sorting the eigenpairs, we ask ourselves, how many principal components we want to have. A useful measure is the so-called \"explained variance\", which can be calculated from the eigenvalues by computing the ratios $\\frac{\\lambda_j}{\\sum_{k=1}^d \\lambda_k}$, where $\\lambda_j$ denotes the $j$-th (sorted) eigenvalue of $\\mathbf{C}$ and $1\\le j \\le d$. The explained variance tells us how much information is contained in each of the principal components. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">Exercise 5 (10 points):</h3>\n",
    "\n",
    "* Write a code that computes the explained variance for each eigenvalue of the iris-dataset example and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous task clearly shows that most of the variance can be explained by the first principal component alone. The second principal component still contains some information, while the third and fourth principal components can safely be neglected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we come to the construction of the $d \\times k$ transformation (and projection) matrix $\\mathbf{W}$ that will be used to transform the Iris data onto the new (and smaller) feature subspace. It basically just consists of the first $k$ eigenvectors. In our Iris dataset example, we choose to reduce the $4$-dimensional feature space to a $k=2$ dimensional one.\n",
    "\n",
    "In the final, step we will use $\\mathbf{W}$ to transform our samples onto the new subspace via the equation\n",
    "$\\mathbf{Y}=\\mathbf{X}\\,\\mathbf{W}$, where $\\mathbf{Y}$ is now an $n \\times k$ matrix (i.e. $150 \\times 2$ in our case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">Exercise 6 (10 points):</h3>\n",
    "\n",
    "* Write a code that computes the transformation matrix $\\mathbf{W}$ and the new representation matrix $\\mathbf{Y}$ for the Iris data set and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the steps that we did so far can be also done with the following implemented algorithm from the sklearn package. The previously computed matrix $\\mathbf{Y}$ should match $\\mathbf{Y}_\\textrm{sklearn}$ from the following code. Note that signs (of individual components) might differ. This is because the directions of the eigenvectors are not fixed and different methods may make different choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nothing to do here\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "sklearn_pca = sklearnPCA(n_components=2)\n",
    "Y_sklearn = sklearn_pca.fit_transform(X_std)\n",
    "print(Y_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As a final step, we provide a visualization of the data in the new coordinate system, consisting of the first two principal components as axes. It should be clearly visible how the different classes are well seperated also in this smaller subspace. The self-implementation and sklearn method should lead to the same results (apart from potential flip of axes directions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nothing to do here\n",
    "X = df.iloc[:,0:4].values\n",
    "y = df.iloc[:,4].values\n",
    "classes = list(set(y))\n",
    "\n",
    "colors = sns.color_palette(n_colors=len(classes)).as_hex()\n",
    "\n",
    "for index,c in enumerate(classes):\n",
    "        plt.scatter(Y[y==c,0], Y[y==c,1], color=colors[index], label=c)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"Downprojected data (self-implementation)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for index,c in enumerate(classes):\n",
    "        plt.scatter(Y_sklearn[y==c,0], Y_sklearn[y==c,1], color=colors[index], label=c)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"Downprojected data (using sklearn)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
